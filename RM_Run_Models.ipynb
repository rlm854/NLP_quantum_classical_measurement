{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccb10d29-3dc4-437e-9d54-6e034d2778db",
   "metadata": {},
   "source": [
    "#### This notebook contains code written by Dr. Rebeckah Fussell that has been modified by Rachel Merrill. ####\n",
    "##### See https://github.com/rkfussell/NLP_quantum_classical_measurement/blob/main/All_data_model.ipynb?short_path=2151e13 for original code #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d5113cb-4829-41fa-8ae5-445267ec83c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import BertModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import calibration_fns as cal\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, balanced_accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080046ab-e271-45cb-ab8d-57a72438b220",
   "metadata": {},
   "source": [
    "## Prepping to rebuild the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76e346e2-05d3-4fbb-a10c-8236e2272862",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "data_folder = cwd + '/2025_spring_data_prepped/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31e074e9-9ab9-4fda-9cb5-1551531b083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None) #prevents columns from truncating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc05ed65-ba60-40cc-a114-bf47ff3f3fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model class\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_classes=2, freeze_bert = False):\n",
    "        #Where we define all the parts of the model\n",
    "        super(Classifier, self).__init__()  # initialize object with everything from the parent class\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 200, num_classes\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Connect these parts and return the output\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf62d33-03a2-4ee0-90a3-e98dcbbbec93",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5ac0576-e4e3-41c5-b485-cc25c5d06d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define new models\n",
    "model_P = Classifier()\n",
    "model_L = Classifier()\n",
    "model_O = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "211014e8-7f7d-41ff-8d3f-1317ffe2359d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=200, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=200, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tell the model we are in inference mode rather than training mode\n",
    "model_P.eval()\n",
    "model_L.eval()\n",
    "model_O.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2cfbb4a-a5fa-4828-862b-0c1d7d6ff6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the saved state dictionary - we are using the map_location=\"cpu\" command because we are loading on a cpu\n",
    "model_P.load_state_dict(torch.load(\"modelP.pth\", map_location=\"cpu\"))\n",
    "model_L.load_state_dict(torch.load(\"modelL.pth\", map_location=\"cpu\"))\n",
    "model_O.load_state_dict(torch.load(\"modelO.pth\", map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f17a06f-e691-4b3f-b50a-302cf3648c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('my_tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e7310fa-0361-4785-bfb8-89e338fc0c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load new data\n",
    "new_data_sources = pd.read_excel(data_folder + \"pre_sources.xlsx\")\n",
    "new_data_morebetter = pd.read_excel(data_folder + \"pre_morebetter.xlsx\")\n",
    "new_data_generic = pd.read_excel(data_folder + \"pre_generic.xlsx\")\n",
    "new_data_twoStudents = pd.read_excel(data_folder + \"pre_twoStudents.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7e8abf8-966f-4c72-b511-d9d5b94258fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes \"extra\" whitespace the model will get confused by, standardizes the text\n",
    "def text_preprocessing_simple(text):\n",
    "    try:\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    except:\n",
    "        pass\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "656ea8e3-d1fe-42bd-a7d9-dfab4e9a4ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_bert(data, tokenizer, max_len):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing_simple(str(sent)),  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=max_len,                  # Max length to truncate/pad\n",
    "            padding='max_length',         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True,      # Return attention mask\n",
    "            truncation = True)\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "832dbcdc-31c4-4edd-8858-e1986f70b1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Dataset class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.data, self.masks = preprocessing_for_bert(df[\"Input\"], tokenizer, MAX_LEN)\n",
    "        self.texts = df[\"Input\"].reset_index(drop=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        mask = self.masks[idx]\n",
    "        text = self.texts[idx]\n",
    "        \n",
    "        return sample, mask, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "923367bd-83af-46b5-9670-cd481be7ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 424 #change this number based on what the trained model says"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ad9e1db-0e2e-49b0-9873-aea2f5b7855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make datasets and dataloaders\n",
    "new_dataset_sources = Dataset(new_data_sources, tokenizer, MAX_LEN)\n",
    "new_dataset_morebetter = Dataset(new_data_morebetter, tokenizer, MAX_LEN)\n",
    "new_dataset_generic = Dataset(new_data_generic, tokenizer, MAX_LEN)\n",
    "new_dataset_twoStudents = Dataset(new_data_twoStudents, tokenizer, MAX_LEN)\n",
    "\n",
    "new_loader_sources = DataLoader(new_dataset_sources, batch_size=16)\n",
    "new_loader_morebetter = DataLoader(new_dataset_morebetter, batch_size=16)\n",
    "new_loader_generic = DataLoader(new_dataset_generic, batch_size=16)\n",
    "new_loader_twoStudents = DataLoader(new_dataset_twoStudents, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb80a98c-193d-4c25-b3d6-7142ff9347bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines a function to run the model\n",
    "def apply_model_to_data_set(model, dataloader):\n",
    "    model.eval()\n",
    "    model=model.to(device)\n",
    "    all_logits = []\n",
    "    \n",
    "    # For each batch in our dataset...\n",
    "    for sents, masks, texts in dataloader:\n",
    "        sents, masks = sents.to(device), masks.to(device)\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(sents, masks)\n",
    "            all_logits.append(logits)\n",
    "            \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    " \n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "    preds = torch.argmax(all_logits, dim=1).flatten().cpu().numpy()\n",
    "    all_logits = all_logits.cpu().numpy()\n",
    "    return all_logits, probs, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "090e0b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available()  else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11d4d7d-bb1c-451a-9e94-e8917797f4fc",
   "metadata": {},
   "source": [
    "## Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6d54a97-a2bd-4b2c-aeaa-fc9ff1f2dcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the L model\n",
    "(sources_logits_L, sources_probs_L, sources_preds_L) = apply_model_to_data_set(model_L, new_loader_sources)\n",
    "(morebetter_logits_L, morebetter_probs_L, morebetter_preds_L) = apply_model_to_data_set(model_L, new_loader_morebetter)\n",
    "(generic_logits_L, generic_probs_L, generic_preds_L) = apply_model_to_data_set(model_L, new_loader_generic)\n",
    "(twoStudents_logits_L, twoStudents_probs_L, twoStudents_preds_L) = apply_model_to_data_set(model_L, new_loader_twoStudents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0ab5f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the P model\n",
    "(sources_logits_P, sources_probs_P, sources_preds_P) = apply_model_to_data_set(model_P, new_loader_sources)\n",
    "(morebetter_logits_P, morebetter_probs_P, morebetter_preds_P) = apply_model_to_data_set(model_P, new_loader_morebetter)\n",
    "(generic_logits_P, generic_probs_P, generic_preds_P) = apply_model_to_data_set(model_P, new_loader_generic)\n",
    "(twoStudents_logits_P, twoStudents_probs_P, twoStudents_preds_P) = apply_model_to_data_set(model_P, new_loader_twoStudents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "add51195",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the O model\n",
    "(sources_logits_O, sources_probs_O, sources_preds_O) = apply_model_to_data_set(model_O, new_loader_sources)\n",
    "(morebetter_logits_O, morebetter_probs_O, morebetter_preds_O) = apply_model_to_data_set(model_O, new_loader_morebetter)\n",
    "(generic_logits_O, generic_probs_O, generic_preds_O) = apply_model_to_data_set(model_O, new_loader_generic)\n",
    "(twoStudents_logits_O, twoStudents_probs_O, twoStudents_preds_O) = apply_model_to_data_set(model_O, new_loader_twoStudents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667974c0-3679-441f-8b0b-0c93dd722c2a",
   "metadata": {},
   "source": [
    "## Saving the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c7f60d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_sources[\"P\"] = sources_preds_P\n",
    "new_data_sources[\"L\"] = sources_preds_L\n",
    "new_data_sources[\"O\"] = sources_preds_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f6a208eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_morebetter[\"P\"] = morebetter_preds_P\n",
    "new_data_morebetter[\"L\"] = morebetter_preds_L\n",
    "new_data_morebetter[\"O\"] = morebetter_preds_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7107fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_generic[\"P\"] = generic_preds_P\n",
    "new_data_generic[\"L\"] = generic_preds_L\n",
    "new_data_generic[\"O\"] = generic_preds_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d4db022",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_twoStudents[\"P\"] = twoStudents_preds_P\n",
    "new_data_twoStudents[\"L\"] = twoStudents_preds_L\n",
    "new_data_twoStudents[\"O\"] = twoStudents_preds_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f25b3dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_sources.to_excel(\"BERT_coded_pre_data_S25/sources_pre_S25.xlsx\")\n",
    "new_data_morebetter.to_excel(\"BERT_coded_pre_data_S25/morebetter_pre_S25.xlsx\")\n",
    "new_data_generic.to_excel(\"BERT_coded_pre_data_S25/generic_pre_S25.xlsx\")\n",
    "new_data_twoStudents.to_excel(\"BERT_coded_pre_data_S25/twoStudents_pre_S25.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27058d0-f594-41bd-8d78-ded3658ce9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to add code to convert preds to labels, add 3 columns to the dataframes (P,L,O) and save to csv\n",
    "#This code already exists locally on the BEEAR Mac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff7c3dc-1e4e-46dc-9b9a-dc87d6af9d53",
   "metadata": {},
   "source": [
    "## Testing the Model Performance: Calibration Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fa9840-d364-4449-8cb5-c077c6f453d4",
   "metadata": {},
   "source": [
    "We want to use some subset of hand-coded data from the new dataset to check the model's performance on this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d11f132-5a10-4884-b77a-323e53a736bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plots and view distribution of C(x)\n",
    "fig, axs = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "df_new_test = pd.DataFrame(data = {\"probs\": probs_val[:,1], \"gt_label\":all_labels_val, \"text\":list(val_dataset_sources_L.texts.values)})\n",
    "axs.hist(df_val[\"probs\"], bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5057d4f1-2680-4b59-b7e3-1ff5487891c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load test data\n",
    "test_data_sources = pd.read_csv(\"test_data_sources.csv\")\n",
    "test_data_morebetter = pd.read_csv(\"test_data_morebetter.csv\")\n",
    "test_data_generic = pd.read_csv(\"test_data_generic.csv\")\n",
    "test_data_twoStudents = pd.read_csv(\"test_data_twoStudents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84af15bd-db81-401c-91fe-7db1ed390ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make datasets and dataloaders\n",
    "test_dataset_sources = Dataset(test_data_sources, tokenizer, MAX_LEN)\n",
    "test_dataset_morebetter = Dataset(test_data_morebetter, tokenizer, MAX_LEN)\n",
    "test_dataset_generic = Dataset(test_data_generic, tokenizer, MAX_LEN)\n",
    "test_dataset_twoStudents = Dataset(test_data_twoStudents, tokenizer, MAX_LEN)\n",
    "\n",
    "test_loader_sources = DataLoader(test_dataset_sources, batch_size=16)\n",
    "test_loader_morebetter = DataLoader(test_dataset_morebetter, batch_size=16)\n",
    "test_loader_generic = DataLoader(test_dataset_generic, batch_size=16)\n",
    "test_loader_twoStudents = DataLoader(test_dataset_twoStudents, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f43fc0e-5305-4c96-af14-66335e91b6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model_to_test_set(model, dataloader):\n",
    "    model.eval()\n",
    "    model=model.to(device)\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    # For each batch in our test set...\n",
    "    for sents, masks, texts, labels in dataloader:\n",
    "        sents, masks = sents.to(device), masks.to(device)\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(sents, masks)\n",
    "            all_logits.append(logits)\n",
    "            all_labels.append(labels)\n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim = 0).cpu().numpy()\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "    preds = torch.argmax(all_logits, dim=1).flatten().cpu().numpy()\n",
    "    all_logits = all_logits.cpu().numpy()\n",
    "    return all_logits, all_labels, probs, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd88f84-a152-4efc-9c9e-190358ff128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run models on data sets\n",
    "\n",
    "(sources_logits_test_L, sources_labels_test_L, sources_probs_test_L, sources_preds_test_L) = apply_model_to_test_set(model_L, test_loader_sources)\n",
    "(morebetter_logits_test_L, morebetter_labels_test_L, morebetter_probs_test_L, morebetter_preds_test_L) = apply_model_to_test_set(model_L, test_loader_morebetter)\n",
    "(generic_logits_test_L, generic_labels_test_L, generic_probs_test_L, generic_preds_test_L) = apply_model_to_data_set(model_L, test_loader_generic)\n",
    "(twoStudents_logits_test_L, twoStudents_labels_test_L, twoStudents_probs_test_L, twoStudents_preds_test_L) = apply_model_to_data_set(model_L, test_loader_twoStudents)\n",
    "\n",
    "(sources_logits_test_P, sources_labels_test_P, sources_probs_test_P, sources_preds_test_P) = apply_model_to_test_set(model_P, test_loader_sources)\n",
    "(morebetter_logits_test_P, morebetter_labels_test_P, morebetter_probs_test_P, morebetter_preds_test_P) = apply_model_to_test_set(model_P, test_loader_morebetter)\n",
    "(generic_logits_test_P, generic_labels_test_P, generic_probs_test_P, generic_preds_test_P) = apply_model_to_data_set(model_P, test_loader_generic)\n",
    "(twoStudents_logits_test_P, twoStudents_labels_test_P, twoStudents_probs_test_P, twoStudents_preds_test_P) = apply_model_to_data_set(model_P, test_loader_twoStudents)\n",
    "\n",
    "(sources_logits_test_O, sources_labels_test_O, sources_probs_test_O, sources_preds_test_O) = apply_model_to_test_set(model_O, test_loader_sources)\n",
    "(morebetter_logits_test_O, morebetter_labels_test_O, morebetter_probs_test_O, morebetter_preds_test_O) = apply_model_to_test_set(model_O, test_loader_morebetter)\n",
    "(generic_logits_test_O, generic_labels_test_O, generic_probs_test_O, generic_preds_test_O) = apply_model_to_data_set(model_O, test_loader_generic)\n",
    "(twoStudents_logits_test_O, twoStudents_labels_test_O, twoStudents_probs_test_O, twoStudents_preds_test_O) = apply_model_to_data_set(model_O, test_loader_twoStudents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d2049b-f481-4441-af2a-b8051c5662fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plots and view distributions of C(x) for Sources\n",
    "fig1, axs1 = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "df_sources_L = pd.DataFrame(data = {\"probs\": sources_probs_test_L[:,1], \"gt_label\":sources_labels_test_L, \"text\":list(test_dataset_sources.texts.values)})\n",
    "axs1.hist(df_sources_L[\"probs\"], bins = 20)\n",
    "\n",
    "fig2, axs2 = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "df_sources_P = pd.DataFrame(data = {\"probs\": sources_probs_test_P[:,1], \"gt_label\":sources_labels_test_P, \"text\":list(test_dataset_sources.texts.values)})\n",
    "axs2.hist(df_sources_L[\"probs\"], bins = 20)\n",
    "\n",
    "fig3, axs3 = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "df_sources_O = pd.DataFrame(data = {\"probs\": sources_probs_test_O[:,1], \"gt_label\":sources_labels_test_O, \"text\":list(test_dataset_sources.texts.values)})\n",
    "axs3.hist(df_sources_L[\"probs\"], bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f12de7-4e36-4e9d-88d3-d81208777be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plots and view distributions of C(x) for More/Better\n",
    "fig1, axs1 = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "df_morebetter_L = pd.DataFrame(data = {\"probs\": morebetter_probs_test_L[:,1], \"gt_label\":morebetter_labels_test_L, \"text\":list(test_dataset_morebetter.texts.values)})\n",
    "axs1.hist(df_morebetter_L[\"probs\"], bins = 20)\n",
    "\n",
    "fig2, axs2 = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "df_morebetter_P = pd.DataFrame(data = {\"probs\": morebetter_probs_test_P[:,1], \"gt_label\":morebetter_labels_test_P, \"text\":list(test_dataset_morebetter.texts.values)})\n",
    "axs2.hist(df_morebetter_L[\"probs\"], bins = 20)\n",
    "\n",
    "fig3, axs3 = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "df_morebetter_O = pd.DataFrame(data = {\"probs\": morebetter_probs_test_O[:,1], \"gt_label\":morebetter_labels_test_O, \"text\":list(test_dataset_morebetter.texts.values)})\n",
    "axs3.hist(df_morebetter_L[\"probs\"], bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c55243f-47fa-477f-bf87-4c9c53345834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plots and view distributions of C(x) for Generic\n",
    "fig1, axs1 = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "df_generic_L = pd.DataFrame(data = {\"probs\": generic_probs_test_L[:,1], \"gt_label\":generic_labels_test_L, \"text\":list(test_dataset_generic.texts.values)})\n",
    "axs1.hist(df_generic_L[\"probs\"], bins = 20)\n",
    "\n",
    "fig2, axs2 = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "df_generic_P = pd.DataFrame(data = {\"probs\": generic_probs_test_P[:,1], \"gt_label\":generic_labels_test_P, \"text\":list(test_dataset_generic.texts.values)})\n",
    "axs2.hist(df_generic_L[\"probs\"], bins = 20)\n",
    "\n",
    "fig3, axs3 = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "df_generic_O = pd.DataFrame(data = {\"probs\": generic_probs_test_O[:,1], \"gt_label\":generic_labels_test_O, \"text\":list(test_dataset_generic.texts.values)})\n",
    "axs3.hist(df_generic_L[\"probs\"], bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80ce153-0ff5-4931-a751-d315428059cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plots and view distributions of C(x) for Two Students\n",
    "fig1, axs1 = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "df_twoStudents_L = pd.DataFrame(data = {\"probs\": twoStudents_probs_test_L[:,1], \"gt_label\":twoStudents_labels_test_L, \"text\":list(test_dataset_twoStudents.texts.values)})\n",
    "axs1.hist(df_twoStudents_L[\"probs\"], bins = 20)\n",
    "\n",
    "fig2, axs2 = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "df_twoStudents_P = pd.DataFrame(data = {\"probs\": twoStudents_probs_test_P[:,1], \"gt_label\":twoStudents_labels_test_P, \"text\":list(test_dataset_twoStudents.texts.values)})\n",
    "axs2.hist(df_twoStudents_L[\"probs\"], bins = 20)\n",
    "\n",
    "fig3, axs3 = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "df_twoStudents_O = pd.DataFrame(data = {\"probs\": twoStudents_probs_test_O[:,1], \"gt_label\":twoStudents_labels_test_O, \"text\":list(test_dataset_twoStudents.texts.values)})\n",
    "axs3.hist(df_twoStudents_L[\"probs\"], bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1637124-d13c-4485-b3db-0829adbe627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get binned calibration curves for Sources\n",
    "binned_cal_curve1 = cal.generate_calibration_curve_binned(df_sources_L, num_bin = 10, binary = True)\n",
    "binned_cal_curve1.plot(show_diagonal=True, filled = False)\n",
    "\n",
    "binned_cal_curve2 = cal.generate_calibration_curve_binned(df_sources_P, num_bin = 10, binary = True)\n",
    "binned_cal_curve2.plot(show_diagonal=True, filled = False)\n",
    "\n",
    "binned_cal_curve2 = cal.generate_calibration_curve_binned(df_sources_O, num_bin = 10, binary = True)\n",
    "binned_cal_curve2.plot(show_diagonal=True, filled = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c036ecb-4d53-4da5-823c-f9b67033cfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get binned calibration curves for More/Better\n",
    "binned_cal_curve1 = cal.generate_calibration_curve_binned(df_morebetter_L, num_bin = 10, binary = True)\n",
    "binned_cal_curve1.plot(show_diagonal=True, filled = False)\n",
    "\n",
    "binned_cal_curve2 = cal.generate_calibration_curve_binned(df_morebetter_P, num_bin = 10, binary = True)\n",
    "binned_cal_curve2.plot(show_diagonal=True, filled = False)\n",
    "\n",
    "binned_cal_curve2 = cal.generate_calibration_curve_binned(df_morebetter_O, num_bin = 10, binary = True)\n",
    "binned_cal_curve2.plot(show_diagonal=True, filled = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fbf23b-61a7-4831-9f63-ed2973e66e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get binned calibration curves for Generic\n",
    "binned_cal_curve1 = cal.generate_calibration_curve_binned(df_generic_L, num_bin = 10, binary = True)\n",
    "binned_cal_curve1.plot(show_diagonal=True, filled = False)\n",
    "\n",
    "binned_cal_curve2 = cal.generate_calibration_curve_binned(df_generic_P, num_bin = 10, binary = True)\n",
    "binned_cal_curve2.plot(show_diagonal=True, filled = False)\n",
    "\n",
    "binned_cal_curve2 = cal.generate_calibration_curve_binned(df_generic_O, num_bin = 10, binary = True)\n",
    "binned_cal_curve2.plot(show_diagonal=True, filled = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc22bd7d-c18a-4299-b210-355c578c8675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get binned calibration curves for Two Students\n",
    "binned_cal_curve1 = cal.generate_calibration_curve_binned(df_twoStudents_L, num_bin = 10, binary = True)\n",
    "binned_cal_curve1.plot(show_diagonal=True, filled = False)\n",
    "\n",
    "binned_cal_curve2 = cal.generate_calibration_curve_binned(df_twoStudents_P, num_bin = 10, binary = True)\n",
    "binned_cal_curve2.plot(show_diagonal=True, filled = False)\n",
    "\n",
    "binned_cal_curve2 = cal.generate_calibration_curve_binned(df_twoStudents_O, num_bin = 10, binary = True)\n",
    "binned_cal_curve2.plot(show_diagonal=True, filled = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b28811d-fc54-4c9c-ab13-0e9e280a3d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get platt calibration curves for Sources and save as png files\n",
    "plt.figure()\n",
    "platt_cal_curve1S = cal.generate_calibration_curve_platt(df_sources_L, binary = True)\n",
    "platt_cal_curve1S.plot(show_diagonal=True, filled = False)\n",
    "plt.savefig(\"cal_curve_sources_L.png\", bbox_inches='tight')\n",
    "#plt.savefig(\"cal_curve.pdf\", bbox_inches='tight')\n",
    "\n",
    "plt.figure()\n",
    "platt_cal_curve2S = cal.generate_calibration_curve_platt(df_sources_P, binary = True)\n",
    "platt_cal_curve2S.plot(show_diagonal=True, filled = False)\n",
    "plt.savefig(\"cal_curve_sources_P.png\", bbox_inches='tight')\n",
    "\n",
    "plt.figure()\n",
    "platt_cal_curve3S = cal.generate_calibration_curve_platt(df_sources_O, binary = True)\n",
    "platt_cal_curve3S.plot(show_diagonal=True, filled = False)\n",
    "plt.savefig(\"cal_curve_sources_O.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b462b7-188a-47e5-be22-4fcd67e9cd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get platt calibration curves for More/Better and save as png files\n",
    "plt.figure()\n",
    "platt_cal_curve1M = cal.generate_calibration_curve_platt(df_morebetter_L, binary = True)\n",
    "platt_cal_curve1M.plot(show_diagonal=True, filled = False)\n",
    "plt.savefig(\"cal_curve_morebetter_L.png\", bbox_inches='tight')\n",
    "#plt.savefig(\"cal_curve.pdf\", bbox_inches='tight')\n",
    "\n",
    "plt.figure()\n",
    "platt_cal_curve2M = cal.generate_calibration_curve_platt(df_morebetter_P, binary = True)\n",
    "platt_cal_curve2M.plot(show_diagonal=True, filled = False)\n",
    "plt.savefig(\"cal_curve_morebetter_P.png\", bbox_inches='tight')\n",
    "\n",
    "plt.figure()\n",
    "platt_cal_curve3M = cal.generate_calibration_curve_platt(df_morebetter_O, binary = True)\n",
    "platt_cal_curve3M.plot(show_diagonal=True, filled = False)\n",
    "plt.savefig(\"cal_curve_morebetter_O.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb5bc02-fd37-4103-9ce0-0e9115ee97b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get platt calibration curves for Generic and save as png files\n",
    "plt.figure()\n",
    "platt_cal_curve1G = cal.generate_calibration_curve_platt(df_generic_L, binary = True)\n",
    "platt_cal_curve1G.plot(show_diagonal=True, filled = False)\n",
    "plt.savefig(\"cal_curve_generic_L.png\", bbox_inches='tight')\n",
    "#plt.savefig(\"cal_curve.pdf\", bbox_inches='tight')\n",
    "\n",
    "plt.figure()\n",
    "platt_cal_curve2G = cal.generate_calibration_curve_platt(df_generic_P, binary = True)\n",
    "platt_cal_curve2G.plot(show_diagonal=True, filled = False)\n",
    "plt.savefig(\"cal_curve_generic_P.png\", bbox_inches='tight')\n",
    "\n",
    "plt.figure()\n",
    "platt_cal_curve3G = cal.generate_calibration_curve_platt(df_generic_O, binary = True)\n",
    "platt_cal_curve3G.plot(show_diagonal=True, filled = False)\n",
    "plt.savefig(\"cal_curve_generic_O.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e239af11-a933-496a-95cf-079db8967a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get platt calibration curves for More/Better and save as png files\n",
    "plt.figure()\n",
    "platt_cal_curve1T = cal.generate_calibration_curve_platt(df_twoStudents_L, binary = True)\n",
    "platt_cal_curve1T.plot(show_diagonal=True, filled = False)\n",
    "plt.savefig(\"cal_curve_twoStudents_L.png\", bbox_inches='tight')\n",
    "#plt.savefig(\"cal_curve.pdf\", bbox_inches='tight')\n",
    "\n",
    "plt.figure()\n",
    "platt_cal_curve2T = cal.generate_calibration_curve_platt(df_twoStudents_P, binary = True)\n",
    "platt_cal_curve2T.plot(show_diagonal=True, filled = False)\n",
    "plt.savefig(\"cal_curve_twoStudents_P.png\", bbox_inches='tight')\n",
    "\n",
    "plt.figure()\n",
    "platt_cal_curve3T = cal.generate_calibration_curve_platt(df_twoStudents_O, binary = True)\n",
    "platt_cal_curve3T.plot(show_diagonal=True, filled = False)\n",
    "plt.savefig(\"cal_curve_twoStudents_O.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59984e4a-2458-4012-a5e5-7b94c635a4b0",
   "metadata": {},
   "source": [
    "## Calculating Performance Metrics: Accuracy, Balanced Accuracy and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716aad1d-70df-4b3b-a084-175e2c7515ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sources metrics\n",
    "sources_performance = pd.DataFrame({\"Code\":[\"P\",\"L\", \"O\"], \n",
    "                                        \"labels\":[sources_labels_test_P,sources_labels_test_L,sources_labels_test_O],\n",
    "                                        \"preds\":[sources_preds_test_P,sources_preds_test_L,sources_preds_test_O],\n",
    "                                        \"probs\":[sources_probs_test_P[:,1],sources_probs_test_L[:,1],sources_probs_test_O[:,1]]\n",
    "                                       })\n",
    "\n",
    "sources_performance[\"Accuracy\"] = sources_performance.apply(lambda row: accuracy_score(row[\"labels\"],row[\"preds\"]), axis = 1)\n",
    "sources_performance[\"Balanced accuracy\"] = sources_performance.apply(lambda row: balanced_accuracy_score(row[\"labels\"],row[\"preds\"]), axis = 1)\n",
    "sources_performance[\"AUC\"] = sources_performance.apply(lambda row: roc_auc_score(row[\"labels\"],row[\"probs\"]), axis = 1)\n",
    "sources_performance = sources_performance.drop([\"labels\",\"preds\",\"probs\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2217d9-9a26-4387-8751-501387a35136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the results\n",
    "sources_performance.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca826cb-d37c-4de2-9a4d-f41cb4fb2611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#More/Better metrics\n",
    "morebetter_performance = pd.DataFrame({\"Code\":[\"P\",\"L\", \"O\"], \n",
    "                                        \"labels\":[morebetter_labels_test_P,morebetter_labels_test_L,morebetter_labels_test_O],\n",
    "                                        \"preds\":[morebetter_preds_test_P,morebetter_preds_test_L,morebetter_preds_test_O],\n",
    "                                        \"probs\":[morebetter_probs_test_P[:,1],morebetter_probs_test_L[:,1],morebetter_probs_test_O[:,1]]\n",
    "                                       })\n",
    "\n",
    "morebetter_performance[\"Accuracy\"] = morebetter_performance.apply(lambda row: accuracy_score(row[\"labels\"],row[\"preds\"]), axis = 1)\n",
    "morebetter_performance[\"Balanced accuracy\"] = morebetter_performance.apply(lambda row: balanced_accuracy_score(row[\"labels\"],row[\"preds\"]), axis = 1)\n",
    "morebetter_performance[\"AUC\"] = morebetter_performance.apply(lambda row: roc_auc_score(row[\"labels\"],row[\"probs\"]), axis = 1)\n",
    "morebetter_performance = morebetter_performance.drop([\"labels\",\"preds\",\"probs\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fd0459-8385-46da-a280-b665588dccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the results\n",
    "morebetter_performance.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a225c9b8-2a87-4511-81cd-5106f94db272",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generic metrics\n",
    "generic_performance = pd.DataFrame({\"Code\":[\"P\",\"L\", \"O\"], \n",
    "                                        \"labels\":[generic_labels_test_P,generic_labels_test_L,generic_labels_test_O],\n",
    "                                        \"preds\":[generic_preds_test_P,generic_preds_test_L,generic_preds_test_O],\n",
    "                                        \"probs\":[generic_probs_test_P[:,1],generic_probs_test_L[:,1],generic_probs_test_O[:,1]]\n",
    "                                       })\n",
    "\n",
    "generic_performance[\"Accuracy\"] = generic_performance.apply(lambda row: accuracy_score(row[\"labels\"],row[\"preds\"]), axis = 1)\n",
    "generic_performance[\"Balanced accuracy\"] = generic_performance.apply(lambda row: balanced_accuracy_score(row[\"labels\"],row[\"preds\"]), axis = 1)\n",
    "generic_performance[\"AUC\"] = generic_performance.apply(lambda row: roc_auc_score(row[\"labels\"],row[\"probs\"]), axis = 1)\n",
    "generic_performance = generic_performance.drop([\"labels\",\"preds\",\"probs\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b74120c-4da4-4090-a5b0-bfe258fb0c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the results\n",
    "generic_performance.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347f6fdc-e962-468e-95e1-c06127524352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two Students metrics\n",
    "twoStudents_performance = pd.DataFrame({\"Code\":[\"P\",\"L\", \"O\"], \n",
    "                                        \"labels\":[twoStudents_labels_test_P,twoStudents_labels_test_L,twoStudents_labels_test_O],\n",
    "                                        \"preds\":[twoStudents_preds_test_P,twoStudents_preds_test_L,twoStudents_preds_test_O],\n",
    "                                        \"probs\":[twoStudents_probs_test_P[:,1],twoStudents_probs_test_L[:,1],twoStudents_probs_test_O[:,1]]\n",
    "                                       })\n",
    "\n",
    "twoStudents_performance[\"Accuracy\"] = twoStudents_performance.apply(lambda row: accuracy_score(row[\"labels\"],row[\"preds\"]), axis = 1)\n",
    "twoStudents_performance[\"Balanced accuracy\"] = twoStudents_performance.apply(lambda row: balanced_accuracy_score(row[\"labels\"],row[\"preds\"]), axis = 1)\n",
    "twoStudents_performance[\"AUC\"] = twoStudents_performance.apply(lambda row: roc_auc_score(row[\"labels\"],row[\"probs\"]), axis = 1)\n",
    "twoStudents_performance = twoStudents_performance.drop([\"labels\",\"preds\",\"probs\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09b42ec-0af4-43f4-a25b-5c82a2dd8777",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the results\n",
    "twoStudents_performance.round(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
